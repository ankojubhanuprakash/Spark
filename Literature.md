* Spark does in memory processing
 * if ram is unavailable, it uses HardDisk_______doubt
 
* Driver - Every spark program has driver, which is master of the program. Resource manager will restart driver if it crashes
* Executor - Which is slave of the prgram, RAM + processor 
 * Local -  a jvm instance is created and it  constitutes both driver and executor. Only good for testing process
 * Cluster - lets say you have four data nodes in a Hadoop cluster, and holding 10GB file. 
             Now to process the file  in spark, Using Yarn we will speicfy number of executors and associated RAM, Driver will push code the to executors simultaneously
* Memory Allocation in an executor, 10% is allocated to system calls, 60% of remaniing 90% is utilised by JVM

* RDD is resilient distributed datastructure            
* A python code is converted to scala and code runs inside JVM


